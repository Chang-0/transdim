{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Temporal Matrix Factorization\n",
    "\n",
    "**Published**: October 8, 2019\n",
    "\n",
    "**Revised**: October 8, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/imputer/BTMF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Temporal Matrix Factorization (BTMF), a fully Bayesian matrix factorization model, on some real-world data sets. To overcome the missing data problem in multivariate time series, BTMF takes into account both low-rank matrix structure and time series autoregression. For an in-depth discussion of BTMF, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Lijun Sun (2019). <b>Bayesian temporal factorization for multidimensional time series prediction</b>. arXiv:1910.06366. <a href=\"https://arxiv.org/pdf/1910.06366.pdf\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "We assume a spatiotemporal setting for multidimensional time series data throughout this work. In general, modern spatiotemporal data sets collected from sensor networks can be organized as matrix time series. For example, we can denote by matrix $Y\\in\\mathbb{R}^{N\\times T}$ a multivariate time series collected from $N$ locations/sensors on $T$ time points, with each row $$\\boldsymbol{y}_{i}=\\left(y_{i,1},y_{i,2},...,y_{i,t-1},y_{i,t},y_{i,t+1},...,y_{i,T}\\right)$$\n",
    "corresponding to the time series collected at location $i$.\n",
    "\n",
    "As mentioned, making accurate predictions on incomplete time series is very challenging, while missing data problem is almost inevitable in real-world applications. Figure 1 illustrates the prediction problem for incomplete time series data. Here we use $(i,t)\\in\\Omega$ to index the observed entries in matrix $Y$.\n",
    "\n",
    "<img src=\"../images/graphical_matrix_time_series.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "> **Figure 1**: Illustration of multivariate time series and the prediction problem in the presence of missing values (green: observed data; white: missing data; red: prediction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "from numpy.random import normal as normrnd\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.linalg import khatri_rao as kr_prod\n",
    "from scipy.stats import wishart\n",
    "from scipy.stats import invwishart\n",
    "from numpy.linalg import solve as solve\n",
    "from numpy.linalg import cholesky as cholesky_lower\n",
    "from scipy.linalg import cholesky as cholesky_upper\n",
    "from scipy.linalg import solve_triangular as solve_ut\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvnrnd_pre(mu, Lambda):\n",
    "    src = normrnd(size = (mu.shape[0],))\n",
    "    return solve_ut(cholesky_upper(Lambda, overwrite_a = True, check_finite = False), \n",
    "                    src, lower = False, check_finite = False, overwrite_b = True) + mu\n",
    "\n",
    "def cov_mat(mat, mat_bar):\n",
    "    mat = mat - mat_bar\n",
    "    return mat.T @ mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0):\n",
    "    \"\"\"Sampling N-by-R factor matrix W and its hyperparameters (mu_w, Lambda_w).\"\"\"\n",
    "    \n",
    "    dim1, rank = W.shape\n",
    "    W_bar = np.mean(W, axis = 0)\n",
    "    temp = dim1 / (dim1 + beta0)\n",
    "    var_mu_hyper = temp * W_bar\n",
    "    var_W_hyper = inv(np.eye(rank) + cov_mat(W, W_bar) + temp * beta0 * np.outer(W_bar, W_bar))\n",
    "    var_Lambda_hyper = wishart.rvs(df = dim1 + rank, scale = var_W_hyper)\n",
    "    var_mu_hyper = mvnrnd_pre(var_mu_hyper, (dim1 + beta0) * var_Lambda_hyper)\n",
    "    \n",
    "    if dim1 * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        var1 = X.T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (var2 @ tau_ind.T).reshape([rank, rank, dim1]) + var_Lambda_hyper[:, :, None]\n",
    "        var4 = var1 @ tau_sparse_mat.T + (var_Lambda_hyper @ var_mu_hyper)[:, None]\n",
    "        for i in range(dim1):\n",
    "            W[i, :] = mvnrnd_pre(solve(var3[:, :, i], var4[:, i]), var3[:, :, i])\n",
    "    elif vargin == 1:\n",
    "        for i in range(dim1):\n",
    "            pos0 = np.where(sparse_mat[i, :] != 0)\n",
    "            Xt = X[pos0[0], :]\n",
    "            var_mu = tau[i] * Xt.T @ sparse_mat[i, pos0[0]] + var_Lambda_hyper @ var_mu_hyper\n",
    "            var_Lambda = tau[i] * Xt.T @ Xt + var_Lambda_hyper\n",
    "            W[i, :] = mvnrnd_pre(solve(var_Lambda, var_mu), var_Lambda)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnrnd(M, U, V):\n",
    "    \"\"\"\n",
    "    Generate matrix normal distributed random matrix.\n",
    "    M is a m-by-n matrix, U is a m-by-m matrix, and V is a n-by-n matrix.\n",
    "    \"\"\"\n",
    "    dim1, dim2 = M.shape\n",
    "    X0 = np.random.randn(dim1, dim2)\n",
    "    P = cholesky_lower(U)\n",
    "    Q = cholesky_lower(V)\n",
    "    \n",
    "    return M + P @ X0 @ Q.T\n",
    "\n",
    "def sample_var_coefficient(X, time_lags):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    tmax = np.max(time_lags)\n",
    "    \n",
    "    Z_mat = X[tmax : dim, :]\n",
    "    Q_mat = np.zeros((dim - tmax, rank * d))\n",
    "    for k in range(d):\n",
    "        Q_mat[:, k * rank : (k + 1) * rank] = X[tmax - time_lags[k] : dim - time_lags[k], :]\n",
    "    var_Psi0 = np.eye(rank * d) + Q_mat.T @ Q_mat\n",
    "    var_Psi = inv(var_Psi0)\n",
    "    var_M = var_Psi @ Q_mat.T @ Z_mat\n",
    "    var_S = np.eye(rank) + Z_mat.T @ Z_mat - var_M.T @ var_Psi0 @ var_M\n",
    "    Sigma = invwishart.rvs(df = rank + dim - tmax, scale = var_S)\n",
    "    \n",
    "    return mnrnd(var_M, var_Psi, Sigma), Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        elif t < tmax:\n",
    "            Qt = np.zeros(rank)\n",
    "            index = list(np.where(t + time_lags >= tmax))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        \n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        if t < tmax:\n",
    "            var3[:, :, t] = var3[:, :, t] - Lambda_x + np.eye(rank)\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_mat, mat_hat, ind):\n",
    "    var_alpha = 1e-6 + 0.5 * np.sum(ind, axis = 1)\n",
    "    var_beta = 1e-6 + 0.5 * np.sum(((sparse_mat - mat_hat) ** 2) * ind, axis = 1)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ar4cast(A, X, Sigma, time_lags, multi_step):\n",
    "    dim, rank = X.shape\n",
    "    d = time_lags.shape[0]\n",
    "    X_new = np.append(X, np.zeros((multi_step, rank)), axis = 0)\n",
    "    for t in range(multi_step):\n",
    "        var = A.T @ X_new[dim + t - time_lags, :].reshape(rank * d)\n",
    "        X_new[dim + t, :] = mvnrnd(var, Sigma)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BTMF Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    d = time_lags.shape[0]\n",
    "    W = init[\"W\"]\n",
    "    X = init[\"X\"]\n",
    "    pos_test = np.where((dense_mat != 0) & (sparse_mat == 0))\n",
    "    dense_test = dense_mat[pos_test]\n",
    "    del dense_mat\n",
    "    ind = sparse_mat != 0\n",
    "    pos_obs = np.where(ind)\n",
    "    tau = np.ones(dim1)\n",
    "    W_plus = np.zeros((dim1, rank, gibbs_iter))\n",
    "    A_plus = np.zeros((rank * d, rank, gibbs_iter))\n",
    "    tau_plus = np.zeros((dim1, gibbs_iter))\n",
    "    Sigma_plus = np.zeros((rank, rank, gibbs_iter))\n",
    "    temp_hat = np.zeros(len(pos_test[0]))\n",
    "    show_iter = 200\n",
    "    mat_hat_plus = np.zeros((dim1, dim2))\n",
    "    X_plus = np.zeros((dim2 + multi_step, rank, gibbs_iter))\n",
    "    mat_new_plus = np.zeros((dim1, multi_step))\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        tau_ind = tau[:, None] * ind\n",
    "        tau_sparse_mat = tau[:, None] * sparse_mat\n",
    "        W = sample_factor_w(tau_sparse_mat, tau_ind, W, X, tau, beta0 = 1, vargin = 0)\n",
    "        A, Sigma = sample_var_coefficient(X, time_lags)\n",
    "        X = sample_factor_x(tau_sparse_mat, tau_ind, time_lags, W, X, A, inv(Sigma))\n",
    "        mat_hat = W @ X.T\n",
    "        tau = sample_precision_tau(sparse_mat, mat_hat, ind)\n",
    "        temp_hat += mat_hat[pos_test]\n",
    "        if (it + 1) % show_iter == 0 and it < burn_iter:\n",
    "            temp_hat = temp_hat / show_iter\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_test, temp_hat)))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_test, temp_hat)))\n",
    "            temp_hat = np.zeros(len(pos_test[0]))\n",
    "            print()\n",
    "        if it + 1 > burn_iter:\n",
    "            W_plus[:, :, it - burn_iter] = W\n",
    "            A_plus[:, :, it - burn_iter] = A\n",
    "            Sigma_plus[:, :, it - burn_iter] = Sigma\n",
    "            tau_plus[:, it - burn_iter] = tau\n",
    "            mat_hat_plus += mat_hat\n",
    "            X0 = ar4cast(A, X, Sigma, time_lags, multi_step)\n",
    "            X_plus[:, :, it - burn_iter] = X0\n",
    "            mat_new_plus += W @ X0[dim2 : dim2 + multi_step, :].T\n",
    "    mat_hat = mat_hat_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_test, mat_hat[:, : dim2][pos_test])))\n",
    "    print()\n",
    "    mat_hat = np.append(mat_hat, mat_new_plus / gibbs_iter, axis = 1)\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W_plus, X_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor_x_partial(tau_sparse_mat, tau_ind, time_lags, W, X, A, Lambda_x, update_horizon):\n",
    "    \"\"\"Sampling T-by-R factor matrix X.\"\"\"\n",
    "    \n",
    "    dim2, rank = X.shape\n",
    "    tmax = np.max(time_lags)\n",
    "    tmin = np.min(time_lags)\n",
    "    d = time_lags.shape[0]\n",
    "    A0 = np.dstack([A] * d)\n",
    "    for k in range(d):\n",
    "        A0[k * rank : (k + 1) * rank, :, k] = 0\n",
    "    mat0 = Lambda_x @ A.T\n",
    "    mat1 = np.einsum('kij, jt -> kit', A.reshape([d, rank, rank]), Lambda_x)\n",
    "    mat2 = np.einsum('kit, kjt -> ij', mat1, A.reshape([d, rank, rank]))\n",
    "    \n",
    "    var1 = W.T\n",
    "    var2 = kr_prod(var1, var1)\n",
    "    var3 = (var2 @ tau_ind).reshape([rank, rank, dim2]) + Lambda_x[:, :, None]\n",
    "    var4 = var1 @ tau_sparse_mat\n",
    "    for t in range(dim2 - update_horizon, dim2):\n",
    "        Mt = np.zeros((rank, rank))\n",
    "        Nt = np.zeros(rank)\n",
    "        Qt = mat0 @ X[t - time_lags, :].reshape(rank * d)\n",
    "        index = list(range(0, d))\n",
    "        if t >= dim2 - tmax and t < dim2 - tmin:\n",
    "            index = list(np.where(t + time_lags < dim2))[0]\n",
    "        if t < dim2 - tmin:\n",
    "            Mt = mat2.copy()\n",
    "            temp = np.zeros((rank * d, len(index)))\n",
    "            n = 0\n",
    "            for k in index:\n",
    "                temp[:, n] = X[t + time_lags[k] - time_lags, :].reshape(rank * d)\n",
    "                n += 1\n",
    "            temp0 = X[t + time_lags[index], :].T - np.einsum('ijk, ik -> jk', A0[:, :, index], temp)\n",
    "            Nt = np.einsum('kij, jk -> i', mat1[index, :, :], temp0)\n",
    "        var3[:, :, t] = var3[:, :, t] + Mt\n",
    "        X[t, :] = mvnrnd_pre(solve(var3[:, :, t], var4[:, t] + Nt + Qt), var3[:, :, t])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BTMF_partial(dense_mat, sparse_mat, init, rank, time_lags, burn_iter, gibbs_iter, multi_step = 1):\n",
    "    \"\"\"Bayesian Temporal Matrix Factorization, BTMF.\"\"\"\n",
    "    \n",
    "    dim1, dim2 = sparse_mat.shape\n",
    "    W_plus = init[\"W_plus\"]\n",
    "    X_plus = init[\"X_plus\"]\n",
    "    A_plus = init[\"A_plus\"]\n",
    "    Sigma_plus = init[\"Sigma_plus\"]\n",
    "    tau_plus = init[\"tau_plus\"]\n",
    "    ind = sparse_mat != 0\n",
    "    pos_obs = np.where(ind)\n",
    "    show_iter = 200\n",
    "    X_new_plus = np.zeros((dim2 + multi_step, rank, gibbs_iter))\n",
    "    mat_new_plus = np.zeros((dim1, multi_step))\n",
    "    back_step = 10 * multi_step\n",
    "    for it in range(gibbs_iter):\n",
    "        tau_ind = tau_plus[:, it][:, None] * ind\n",
    "        tau_sparse_mat = tau_plus[:, it][:, None] * sparse_mat\n",
    "        X = sample_factor_x_partial(tau_sparse_mat, tau_ind, time_lags, W_plus[:, :, it], \n",
    "                                    X_plus[:, :, it], A_plus[:, :, it], inv(Sigma_plus[:, :, it]), back_step)\n",
    "        X0 = ar4cast(A_plus[:, :, it], X, Sigma_plus[:, :, it], time_lags, multi_step)\n",
    "        X_new_plus[:, :, it] = X0\n",
    "        mat_new_plus += W_plus[:, :, it] @ X0[- multi_step :, :].T\n",
    "    mat_hat = mat_new_plus / gibbs_iter\n",
    "    mat_hat[mat_hat < 0] = 0\n",
    "    \n",
    "    return mat_hat, W_plus, X_new_plus, A_plus, Sigma_plus, tau_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "def BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter):\n",
    "    T = dense_mat.shape[1]\n",
    "    start_time = T - pred_step\n",
    "    dim1 = dense_mat.shape[0]\n",
    "    mat_hat = np.zeros((dim1, pred_step))\n",
    "    \n",
    "    max_count = int(pred_step / multi_step)\n",
    "    f = IntProgress(min = 0, max = max_count) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "    for t in range(int(np.ceil(pred_step / multi_step))):\n",
    "        if t == 0:\n",
    "            init = {\"W\": 0.1 * np.random.randn(dim1, rank), \"X\": 0.1 * np.random.randn(start_time, rank)}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF(dense_mat[:, 0 : start_time], \n",
    "                sparse_mat[:, 0 : start_time], init, rank, time_lags, burn_iter, gibbs_iter, multi_step)\n",
    "        else:\n",
    "            init = {\"W_plus\": W, \"X_plus\": X_new, \"A_plus\": A, \"Sigma_plus\": Sigma, \"tau_plus\": tau}\n",
    "            mat, W, X_new, A, Sigma, tau = BTMF_partial(dense_mat[:, 0 : start_time + t * multi_step], \n",
    "                sparse_mat[:, 0 : start_time + t * multi_step], init, rank, time_lags, \n",
    "                                                        burn_iter, gibbs_iter, multi_step)\n",
    "        mat_hat[:, t * multi_step : (t + 1) * multi_step] = mat[:, - multi_step :]\n",
    "        f.value = t\n",
    "    small_dense_mat = dense_mat[:, start_time : dense_mat.shape[1]]\n",
    "    pos = np.where(small_dense_mat != 0)\n",
    "    print('Prediction MAPE: {:.6}'.format(compute_mape(small_dense_mat[pos], mat_hat[pos])))\n",
    "    print('Prediction RMSE: {:.6}'.format(compute_rmse(small_dense_mat[pos], mat_hat[pos])))\n",
    "    print()\n",
    "    return mat_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Guangzhou Speed Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_matrix.mat')['random_matrix']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "binary_mat = binary_tensor.reshape([binary_tensor.shape[0], binary_tensor.shape[1] * binary_tensor.shape[2]])\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 144\n",
    "- Time lags: {1, 2, 144, 144 + 1, 144 + 2, 7 * 144, 7 * 144 + 1, 7 * 144 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time horizon (delta) = 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73899338308d4d958e2b63fd9d53a6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=504)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.103107\n",
      "RMSE: 4.34289\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.101911\n",
      "RMSE: 4.32043\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.101783\n",
      "RMSE: 4.3144\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.101757\n",
      "RMSE: 4.31506\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.101788\n",
      "RMSE: 4.31628\n",
      "\n",
      "Imputation MAPE: 0.101783\n",
      "Imputation RMSE: 4.31732\n",
      "\n",
      "Prediction MAPE: 0.113524\n",
      "Prediction RMSE: 4.55913\n",
      "\n",
      "Running time: 6637 seconds\n",
      "\n",
      "Prediction time horizon (delta) = 4.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2c51cab39446d59e7ca12cc832c56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=252)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 200\n",
      "MAPE: 0.102962\n",
      "RMSE: 4.34077\n",
      "\n",
      "Iter: 400\n",
      "MAPE: 0.101998\n",
      "RMSE: 4.32219\n",
      "\n",
      "Iter: 600\n",
      "MAPE: 0.101824\n",
      "RMSE: 4.31483\n",
      "\n",
      "Iter: 800\n",
      "MAPE: 0.101794\n",
      "RMSE: 4.3132\n",
      "\n",
      "Iter: 1000\n",
      "MAPE: 0.101797\n",
      "RMSE: 4.31296\n",
      "\n",
      "Imputation MAPE: 0.101828\n",
      "Imputation RMSE: 4.31437\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 144\n",
    "time_lags = np.array([1, 2, 3, 144, 145, 146, 7 * 144, 7 * 144 + 1, 7 * 144 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 144\n",
    "- Time lags: {1, 2, 144, 144 + 1, 144 + 2, 7 * 144, 7 * 144 + 1, 7 * 144 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 144\n",
    "time_lags = np.array([1, 2, 3, 144, 145, 146, 7 * 144, 7 * 144 + 1, 7 * 144 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $214\\times 61\\times 144$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Guangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.6\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 144\n",
    "- Time lags: {1, 2, 144, 144 + 1, 144 + 2, 7 * 144, 7 * 144 + 1, 7 * 144 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 144\n",
    "time_lags = np.array([1, 2, 3, 144, 145, 146, 7 * 144, 7 * 144 + 1, 7 * 144 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Hangzhou Flow Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $80\\times 25\\times 108$ (metro station, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/tensor.mat')['tensor']\n",
    "random_matrix = scipy.io.loadmat('../datasets/Hangzhou-data-set/random_matrix.mat')['random_matrix']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(random_matrix[i1, i2] + 0.5 - missing_rate)\n",
    "binary_mat = binary_tensor.reshape([binary_tensor.shape[0], binary_tensor.shape[1] * binary_tensor.shape[2]])\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 108\n",
    "- Time lags: {1, 2, 108, 108 + 1, 108 + 2, 7 * 108, 7 * 108 + 1, 7 * 108 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 108\n",
    "time_lags = np.array([1, 2, 3, 108, 109, 110, 7 * 108, 7 * 108 + 1, 7 * 108 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $80\\times 25\\times 108$ (metro station, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 108\n",
    "- Time lags: {1, 2, 108, 108 + 1, 108 + 2, 7 * 108, 7 * 108 + 1, 7 * 108 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 108\n",
    "time_lags = np.array([1, 2, 3, 108, 109, 110, 7 * 108, 7 * 108 + 1, 7 * 108 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $80\\times 25\\times 108$ (metro station, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/tensor.mat')['tensor']\n",
    "random_tensor = scipy.io.loadmat('../datasets/Hangzhou-data-set/random_tensor.mat')['random_tensor']\n",
    "dense_mat = tensor.reshape([tensor.shape[0], tensor.shape[1] * tensor.shape[2]])\n",
    "missing_rate = 0.6\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = (np.round(random_tensor + 0.5 - missing_rate)\n",
    "              .reshape([random_tensor.shape[0], random_tensor.shape[1] * random_tensor.shape[2]]))\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 108\n",
    "- Time lags: {1, 2, 108, 108 + 1, 108 + 2, 7 * 108, 7 * 108 + 1, 7 * 108 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 108\n",
    "time_lags = np.array([1, 2, 3, 108, 109, 110, 7 * 108, 7 * 108 + 1, 7 * 108 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Seattle Speed Data\n",
    "\n",
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $323\\times 28\\times 288$ (road segment, day, time of day)\n",
    "- Non-random missing (NM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dense_mat = pd.read_csv('../datasets/Seattle-data-set/mat.csv', index_col = 0)\n",
    "NM_mat = pd.read_csv('../datasets/Seattle-data-set/NM_mat.csv', index_col = 0)\n",
    "dense_mat = dense_mat.values\n",
    "NM_mat = NM_mat.values\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_tensor = np.zeros((dense_mat.shape[0], 28, 288))\n",
    "for i1 in range(binary_tensor.shape[0]):\n",
    "    for i2 in range(binary_tensor.shape[1]):\n",
    "        binary_tensor[i1, i2, :] = np.round(NM_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_tensor.reshape([dense_mat.shape[0], dense_mat.shape[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 288\n",
    "- Time lags: {1, 2, 288, 288 + 1, 288 + 2, 7 * 288, 7 * 288 + 1, 7 * 288 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 288\n",
    "time_lags = np.array([1, 2, 3, 288, 289, 290, 7 * 288, 7 * 288 + 1, 7 * 288 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $323\\times 28\\times 288$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 40% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dense_mat = pd.read_csv('../datasets/Seattle-data-set/mat.csv', index_col = 0)\n",
    "RM_mat = pd.read_csv('../datasets/Seattle-data-set/RM_mat.csv', index_col = 0)\n",
    "dense_mat = dense_mat.values\n",
    "RM_mat = RM_mat.values\n",
    "missing_rate = 0.4\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = np.round(RM_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 288\n",
    "- Time lags: {1, 2, 288, 288 + 1, 288 + 2, 7 * 288, 7 * 288 + 1, 7 * 288 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 288\n",
    "time_lags = np.array([1, 2, 3, 288, 289, 290, 7 * 288, 7 * 288 + 1, 7 * 288 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario setting**:\n",
    "\n",
    "- Tensor size: $323\\times 28\\times 288$ (road segment, day, time of day)\n",
    "- Random missing (RM)\n",
    "- 60% missing rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dense_mat = pd.read_csv('../datasets/Seattle-data-set/mat.csv', index_col = 0)\n",
    "RM_mat = pd.read_csv('../datasets/Seattle-data-set/RM_mat.csv', index_col = 0)\n",
    "dense_mat = dense_mat.values\n",
    "RM_mat = RM_mat.values\n",
    "missing_rate = 0.6\n",
    "\n",
    "## Random missing (RM)\n",
    "binary_mat = np.round(RM_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 288\n",
    "- Time lags: {1, 2, 288, 288 + 1, 288 + 2, 7 * 288, 7 * 288 + 1, 7 * 288 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 288\n",
    "time_lags = np.array([1, 2, 3, 288, 289, 290, 7 * 288, 7 * 288 + 1, 7 * 288 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 12, 18, 24, 30, 36, 42, 48, 54]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on London Movement Speed Data\n",
    "\n",
    "London movement speed data set is is a city-wide hourly traffic speeddataset collected in London.\n",
    "\n",
    "- Collected from 200,000+ road segments.\n",
    "- 720 time points in April 2019.\n",
    "- 73% missing values in the original data.\n",
    "\n",
    "|  Observation rate | $>90\\%$ | $>80\\%$ | $>70\\%$ | $>60\\%$ | $>50\\%$ |\n",
    "|:------------------|--------:|--------:|--------:|--------:|--------:|\n",
    "|**Number of roads**|  17,666 |  27,148 |  35,912 |  44,352 |  52,727 |\n",
    "\n",
    "\n",
    "If want to test on the full dataset, you could consider the following setting for masking observations as missing values. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "mask_rate = 0.20\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "pos_obs = np.where(dense_mat != 0)\n",
    "num = len(pos_obs[0])\n",
    "sample_ind = np.random.choice(num, size = int(mask_rate * num), replace = False)\n",
    "sparse_mat = dense_mat.copy()\n",
    "sparse_mat[pos_obs[0][sample_ind], pos_obs[1][sample_ind]] = 0\n",
    "```\n",
    "\n",
    "Notably, you could also consider to evaluate the model on a subset of the data with the following setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.4\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Non-random missing (NM)\n",
    "binary_mat = np.zeros(dense_mat.shape)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], 30)\n",
    "for i1 in range(dense_mat.shape[0]):\n",
    "    for i2 in range(30):\n",
    "        binary_mat[i1, i2 * 24 : (i2 + 1) * 24] = np.round(random_mat[i1, i2] + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.4\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "missing_rate = 0.6\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "binary_mat = dense_mat.copy()\n",
    "binary_mat[binary_mat != 0] = 1\n",
    "pos = np.where(np.sum(binary_mat, axis = 1) > 0.7 * binary_mat.shape[1])\n",
    "dense_mat = dense_mat[pos[0], :]\n",
    "\n",
    "## Random missing (RM)\n",
    "random_mat = np.random.rand(dense_mat.shape[0], dense_mat.shape[1])\n",
    "binary_mat = np.round(random_mat + 0.5 - missing_rate)\n",
    "sparse_mat = np.multiply(dense_mat, binary_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setting**:\n",
    "\n",
    "- Low rank: 10\n",
    "- Total (rolling) prediction horizons: 7 * 24\n",
    "- Time lags: {1, 2, 24, 24 + 1, 24 + 2, 7 * 24, 7 * 24 + 1, 7 * 24 + 2}\n",
    "- The number of burn-in iterations: 1000\n",
    "- The number of Gibbs iterations: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rank = 10\n",
    "pred_step = 7 * 24\n",
    "time_lags = np.array([1, 2, 3, 24, 25, 26, 7 * 24, 7 * 24 + 1, 7 * 24 + 2])\n",
    "burn_iter = 1000\n",
    "gibbs_iter = 200\n",
    "for multi_step in [2, 4, 6, 7, 8, 9, 10, 11, 12, 13, 14]:\n",
    "    start = time.time()\n",
    "    print('Prediction time horizon (delta) = {}.'.format(multi_step))\n",
    "    mat_hat = BTMF_forecast(dense_mat, sparse_mat, pred_step, multi_step, rank, time_lags, burn_iter, gibbs_iter)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
