{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Gaussian CP decomposition\n",
    "\n",
    "**Published**: September 30, 2020\n",
    "\n",
    "**Author**: Xinyu Chen [[**GitHub homepage**](https://github.com/xinychen)]\n",
    "\n",
    "**Download**: This Jupyter notebook is at our GitHub repository. If you want to evaluate the code, please download the notebook from the [**transdim**](https://github.com/xinychen/transdim/blob/master/imputer/BTMF.ipynb) repository.\n",
    "\n",
    "This notebook shows how to implement the Bayesian Gaussian CP decomposition (BGCP) model on some real-world data sets. In the following, we will discuss:\n",
    "\n",
    "- What the Bayesian Gaussian CP decomposition is.\n",
    "\n",
    "- How to implement BGCP mainly using Python `numpy` with high efficiency.\n",
    "\n",
    "- How to make imputation on some real-world spatiotemporal datasets.\n",
    "\n",
    "To overcome the problem of missing values within multivariate time series data, this model takes into account low-rank tensor structure by folding data along day dimension. For an in-depth discussion of BGCP, please see [1].\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font color=\"black\">\n",
    "<b>[1]</b> Xinyu Chen, Zhaocheng He, Lijun Sun (2019). <b>A Bayesian tensor decomposition approach for spatiotemporal traffic data imputation</b>. Transportation Research Part C: Emerging Technologies, 98: 73-84. <a href=\"https://doi.org/10.1016/j.trc.2018.11.003\" title=\"PDF\"><b>[PDF]</b></a> \n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary dependencies. We will make use of `numpy` and `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import multivariate_normal as mvnrnd\n",
    "from scipy.stats import wishart\n",
    "from numpy.linalg import inv as inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Computation Concepts\n",
    "\n",
    "### 1) Kronecker product\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "Given two matrices $A\\in\\mathbb{R}^{m_1\\times n_1}$ and $B\\in\\mathbb{R}^{m_2\\times n_2}$, then, the **Kronecker product** between these two matrices is defined as\n",
    "\n",
    "$$A\\otimes B=\\left[ \\begin{array}{cccc} a_{11}B & a_{12}B & \\cdots & a_{1m_2}B \\\\ a_{21}B & a_{22}B & \\cdots & a_{2m_2}B \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m_11}B & a_{m_12}B & \\cdots & a_{m_1m_2}B \\\\ \\end{array} \\right]$$\n",
    "where the symbol $\\otimes$ denotes Kronecker product, and the size of resulted $A\\otimes B$ is $(m_1m_2)\\times (n_1n_2)$ (i.e., $m_1\\times m_2$ columns and $n_1\\times n_2$ rows).\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "If $A=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]$ and $B=\\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10 \\\\ \\end{array} \\right]$, then, we have\n",
    "\n",
    "$$A\\otimes B=\\left[ \\begin{array}{cc} 1\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] & 2\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] \\\\ 3\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] & 4\\times \\left[ \\begin{array}{ccc} 5 & 6 & 7\\\\ 8 & 9 & 10\\\\ \\end{array} \\right] \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cccccc} 5 & 6 & 7 & 10 & 12 & 14 \\\\ 8 & 9 & 10 & 16 & 18 & 20 \\\\ 15 & 18 & 21 & 20 & 24 & 28 \\\\ 24 & 27 & 30 & 32 & 36 & 40 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 6}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Khatri-Rao product (`kr_prod`)\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "Given two matrices $A=\\left( \\boldsymbol{a}_1,\\boldsymbol{a}_2,...,\\boldsymbol{a}_r \\right)\\in\\mathbb{R}^{m\\times r}$ and $B=\\left( \\boldsymbol{b}_1,\\boldsymbol{b}_2,...,\\boldsymbol{b}_r \\right)\\in\\mathbb{R}^{n\\times r}$ with same number of columns, then, the **Khatri-Rao product** (or **column-wise Kronecker product**) between $A$ and $B$ is given as follows,\n",
    "\n",
    "$$A\\odot B=\\left( \\boldsymbol{a}_1\\otimes \\boldsymbol{b}_1,\\boldsymbol{a}_2\\otimes \\boldsymbol{b}_2,...,\\boldsymbol{a}_r\\otimes \\boldsymbol{b}_r \\right)\\in\\mathbb{R}^{(mn)\\times r},$$\n",
    "where the symbol $\\odot$ denotes Khatri-Rao product, and $\\otimes$ denotes Kronecker product.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "If $A=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]=\\left( \\boldsymbol{a}_1,\\boldsymbol{a}_2 \\right) $ and $B=\\left[ \\begin{array}{cc} 5 & 6 \\\\ 7 & 8 \\\\ 9 & 10 \\\\ \\end{array} \\right]=\\left( \\boldsymbol{b}_1,\\boldsymbol{b}_2 \\right) $, then, we have\n",
    "\n",
    "$$A\\odot B=\\left( \\boldsymbol{a}_1\\otimes \\boldsymbol{b}_1,\\boldsymbol{a}_2\\otimes \\boldsymbol{b}_2 \\right) $$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cc} \\left[ \\begin{array}{c} 1 \\\\ 3 \\\\ \\end{array} \\right]\\otimes \\left[ \\begin{array}{c} 5 \\\\ 7 \\\\ 9 \\\\ \\end{array} \\right] & \\left[ \\begin{array}{c} 2 \\\\ 4 \\\\ \\end{array} \\right]\\otimes \\left[ \\begin{array}{c} 6 \\\\ 8 \\\\ 10 \\\\ \\end{array} \\right] \\\\ \\end{array} \\right]$$\n",
    "\n",
    "$$=\\left[ \\begin{array}{cc} 5 & 12 \\\\ 7 & 16 \\\\ 9 & 20 \\\\ 15 & 24 \\\\ 21 & 32 \\\\ 27 & 40 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{6\\times 2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kr_prod(a, b):\n",
    "    return np.einsum('ir, jr -> ijr', a, b).reshape(a.shape[0] * b.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 12]\n",
      " [ 7 16]\n",
      " [ 9 20]\n",
      " [15 24]\n",
      " [21 32]\n",
      " [27 40]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8], [9, 10]])\n",
    "print(kr_prod(A, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) CP decomposition\n",
    "\n",
    "#### CP Combination (`cp_combine`)\n",
    "\n",
    "- **Definition**:\n",
    "\n",
    "The CP decomposition factorizes a tensor into a sum of outer products of vectors. For example, for a third-order tensor $\\mathcal{Y}\\in\\mathbb{R}^{m\\times n\\times f}$, the CP decomposition can be written as\n",
    "\n",
    "$$\\hat{\\mathcal{Y}}=\\sum_{s=1}^{r}\\boldsymbol{u}_{s}\\circ\\boldsymbol{v}_{s}\\circ\\boldsymbol{x}_{s},$$\n",
    "or element-wise,\n",
    "\n",
    "$$\\hat{y}_{ijt}=\\sum_{s=1}^{r}u_{is}v_{js}x_{ts},\\forall (i,j,t),$$\n",
    "where vectors $\\boldsymbol{u}_{s}\\in\\mathbb{R}^{m},\\boldsymbol{v}_{s}\\in\\mathbb{R}^{n},\\boldsymbol{x}_{s}\\in\\mathbb{R}^{f}$ are columns of factor matrices $U\\in\\mathbb{R}^{m\\times r},V\\in\\mathbb{R}^{n\\times r},X\\in\\mathbb{R}^{f\\times r}$, respectively. The symbol $\\circ$ denotes vector outer product.\n",
    "\n",
    "- **Example**:\n",
    "\n",
    "Given matrices $U=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{2\\times 2}$, $V=\\left[ \\begin{array}{cc} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{3\\times 2}$ and $X=\\left[ \\begin{array}{cc} 1 & 5 \\\\ 2 & 6 \\\\ 3 & 7 \\\\ 4 & 8 \\\\ \\end{array} \\right]\\in\\mathbb{R}^{4\\times 2}$, then if $\\hat{\\mathcal{Y}}=\\sum_{s=1}^{r}\\boldsymbol{u}_{s}\\circ\\boldsymbol{v}_{s}\\circ\\boldsymbol{x}_{s}$, then, we have\n",
    "\n",
    "$$\\hat{Y}_1=\\hat{\\mathcal{Y}}(:,:,1)=\\left[ \\begin{array}{ccc} 31 & 42 & 65 \\\\ 63 & 86 & 135 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_2=\\hat{\\mathcal{Y}}(:,:,2)=\\left[ \\begin{array}{ccc} 38 & 52 & 82 \\\\ 78 & 108 & 174 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_3=\\hat{\\mathcal{Y}}(:,:,3)=\\left[ \\begin{array}{ccc} 45 & 62 & 99 \\\\ 93 & 130 & 213 \\\\ \\end{array} \\right],$$\n",
    "$$\\hat{Y}_4=\\hat{\\mathcal{Y}}(:,:,4)=\\left[ \\begin{array}{ccc} 52 & 72 & 116 \\\\ 108 & 152 & 252 \\\\ \\end{array} \\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_combine(var):\n",
    "    return np.einsum('is, js, ts -> ijt', var[0], var[1], var[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 31  38  45  52]\n",
      "  [ 42  52  62  72]\n",
      "  [ 65  82  99 116]]\n",
      "\n",
      " [[ 63  78  93 108]\n",
      "  [ 86 108 130 152]\n",
      "  [135 174 213 252]]]\n",
      "\n",
      "tensor size:\n",
      "(2, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "factor = [np.array([[1, 2], [3, 4]]), np.array([[1, 3], [2, 4], [5, 6]]), \n",
    "          np.array([[1, 5], [2, 6], [3, 7], [4, 8]])]\n",
    "print(cp_combine(factor))\n",
    "print()\n",
    "print('tensor size:')\n",
    "print(cp_combine(factor).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Tensor Unfolding (`ten2mat`)\n",
    "\n",
    "Using numpy reshape to perform 3rd rank tensor unfold operation. [[**link**](https://stackoverflow.com/questions/49970141/using-numpy-reshape-to-perform-3rd-rank-tensor-unfold-operation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size:\n",
      "(3, 2, 4)\n",
      "original tensor:\n",
      "[[[ 1  2  3  4]\n",
      "  [ 3  4  5  6]]\n",
      "\n",
      " [[ 5  6  7  8]\n",
      "  [ 7  8  9 10]]\n",
      "\n",
      " [[ 9 10 11 12]\n",
      "  [11 12 13 14]]]\n",
      "\n",
      "(1) mode-1 tensor unfolding:\n",
      "[[ 1  3  2  4  3  5  4  6]\n",
      " [ 5  7  6  8  7  9  8 10]\n",
      " [ 9 11 10 12 11 13 12 14]]\n",
      "\n",
      "(2) mode-2 tensor unfolding:\n",
      "[[ 1  5  9  2  6 10  3  7 11  4  8 12]\n",
      " [ 3  7 11  4  8 12  5  9 13  6 10 14]]\n",
      "\n",
      "(3) mode-3 tensor unfolding:\n",
      "[[ 1  5  9  3  7 11]\n",
      " [ 2  6 10  4  8 12]\n",
      " [ 3  7 11  5  9 13]\n",
      " [ 4  8 12  6 10 14]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[[1, 2, 3, 4], [3, 4, 5, 6]], \n",
    "              [[5, 6, 7, 8], [7, 8, 9, 10]], \n",
    "              [[9, 10, 11, 12], [11, 12, 13, 14]]])\n",
    "print('tensor size:')\n",
    "print(X.shape)\n",
    "print('original tensor:')\n",
    "print(X)\n",
    "print()\n",
    "print('(1) mode-1 tensor unfolding:')\n",
    "print(ten2mat(X, 0))\n",
    "print()\n",
    "print('(2) mode-2 tensor unfolding:')\n",
    "print(ten2mat(X, 1))\n",
    "print()\n",
    "print('(3) mode-3 tensor unfolding:')\n",
    "print(ten2mat(X, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Computing Covariance Matrix (`cov_mat`)\n",
    "\n",
    "For any matrix $X\\in\\mathbb{R}^{m\\times n}$, `cov_mat` can return a $n\\times n$ covariance matrix for special use in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_mat(mat):\n",
    "    dim1, dim2 = mat.shape\n",
    "    new_mat = np.zeros((dim2, dim2))\n",
    "    mat_bar = np.mean(mat, axis = 0)\n",
    "    for i in range(dim1):\n",
    "        new_mat += np.einsum('i, j -> ij', mat[i, :] - mat_bar, mat[i, :] - mat_bar)\n",
    "    return new_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Gaussian CP decomposition (BGCP)\n",
    "\n",
    "### 1) Model Description\n",
    "\n",
    "#### Gaussian assumption\n",
    "\n",
    "Given a matrix $\\mathcal{Y}\\in\\mathbb{R}^{m\\times n\\times f}$ which suffers from missing values, then the factorization can be applied to reconstruct the missing values within $\\mathcal{Y}$ by\n",
    "\n",
    "$$y_{ijt}\\sim\\mathcal{N}\\left(\\sum_{s=1}^{r}u_{is} v_{js} x_{ts},\\tau^{-1}\\right),\\forall (i,j,t),$$\n",
    "where vectors $\\boldsymbol{u}_{s}\\in\\mathbb{R}^{m},\\boldsymbol{v}_{s}\\in\\mathbb{R}^{n},\\boldsymbol{x}_{s}\\in\\mathbb{R}^{f}$ are columns of latent factor matrices, and $u_{is},v_{js},x_{ts}$ are their elements. The precision term $\\tau$ is an inverse of Gaussian variance.\n",
    "\n",
    "#### Bayesian framework\n",
    "\n",
    "Based on the Gaussian assumption over tensor elements $y_{ijt},(i,j,t)\\in\\Omega$ (where $\\Omega$ is a index set indicating observed tensor elements), the conjugate priors of model parameters (i.e., latent factors and precision term) and hyperparameters are given as\n",
    "\n",
    "$$\\boldsymbol{u}_{i}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{u},\\Lambda_{u}^{-1}\\right),\\forall i,$$\n",
    "$$\\boldsymbol{v}_{j}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{v},\\Lambda_{v}^{-1}\\right),\\forall j,$$\n",
    "$$\\boldsymbol{x}_{t}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{x},\\Lambda_{x}^{-1}\\right),\\forall t,$$\n",
    "$$\\tau\\sim\\text{Gamma}\\left(a_0,b_0\\right),$$\n",
    "$$\\boldsymbol{\\mu}_{u}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,\\left(\\beta_0\\Lambda_u\\right)^{-1}\\right),\\Lambda_u\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right),$$\n",
    "$$\\boldsymbol{\\mu}_{v}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,\\left(\\beta_0\\Lambda_v\\right)^{-1}\\right),\\Lambda_v\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right),$$\n",
    "$$\\boldsymbol{\\mu}_{x}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_0,\\left(\\beta_0\\Lambda_x\\right)^{-1}\\right),\\Lambda_x\\sim\\mathcal{W}\\left(W_0,\\nu_0\\right).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Posterior Inference\n",
    "\n",
    "In the following, we will apply Gibbs sampling to implement our Bayesian inference for the matrix factorization task.\n",
    "\n",
    "#### - Sampling latent factors $\\boldsymbol{u}_{i},i\\in\\left\\{1,2,...,m\\right\\}$\n",
    "\n",
    "Draw $\\boldsymbol{u}_{i}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_i^{*},(\\Lambda_{i}^{*})^{-1}\\right)$ with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{i}^{*}=\\left(\\Lambda_{i}^{*}\\right)^{-1}\\left\\{\\tau\\sum_{j,t:(i,j,t)\\in\\Omega}y_{ijt}\\left(\\boldsymbol{v}_{j}\\circledast\\boldsymbol{x}_{t}\\right)+\\Lambda_u\\boldsymbol{\\mu}_u\\right\\},$$\n",
    "\n",
    "$$\\Lambda_{i}^{*}=\\tau\\sum_{j,t:(i,j,t)\\in\\Omega}\\left(\\boldsymbol{v}_{j}\\circledast\\boldsymbol{x}_{t}\\right)\\left(\\boldsymbol{v}_{j}\\circledast\\boldsymbol{x}_{t}\\right)^{T}+\\Lambda_u.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sampling latent factors $\\boldsymbol{v}_{j},j\\in\\left\\{1,2,...,n\\right\\}$\n",
    "\n",
    "Draw $\\boldsymbol{v}_{j}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_j^{*},(\\Lambda_{j}^{*})^{-1}\\right)$ with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{j}^{*}=\\left(\\Lambda_{j}^{*}\\right)^{-1}\\left\\{\\tau\\sum_{i,t:(i,j,t)\\in\\Omega}y_{ijt}\\left(\\boldsymbol{u}_{i}\\circledast\\boldsymbol{x}_{t}\\right)+\\Lambda_v\\boldsymbol{\\mu}_v\\right\\}$$\n",
    "\n",
    "$$\\Lambda_{j}^{*}=\\tau\\sum_{i,t:(i,j,t)\\in\\Omega}\\left(\\boldsymbol{u}_{i}\\circledast\\boldsymbol{x}_{t}\\right)\\left(\\boldsymbol{u}_{i}\\circledast\\boldsymbol{x}_{t}\\right)^{T}+\\Lambda_v.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sampling latent factors $\\boldsymbol{x}_{t},t\\in\\left\\{1,2,...,f\\right\\}$\n",
    "\n",
    "Draw $\\boldsymbol{x}_{t}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_t^{*},(\\Lambda_{t}^{*})^{-1}\\right)$ with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{t}^{*}=\\left(\\Lambda_{t}^{*}\\right)^{-1}\\left\\{\\tau\\sum_{i,j:(i,j,t)\\in\\Omega}y_{ijt}\\left(\\boldsymbol{u}_{i}\\circledast\\boldsymbol{v}_{j}\\right)+\\Lambda_x\\boldsymbol{\\mu}_x\\right\\}$$\n",
    "\n",
    "$$\\Lambda_{t}^{*}=\\tau\\sum_{i,j:(i,j,t)\\in\\Omega}\\left(\\boldsymbol{u}_{i}\\circledast\\boldsymbol{v}_{j}\\right)\\left(\\boldsymbol{u}_{i}\\circledast\\boldsymbol{v}_{j}\\right)^{T}+\\Lambda_x.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_factor(sparse_tensor, binary_tensor, tau, factor, var_mu_hyper, var_Lambda_hyper, k, vargin = 0):\n",
    "    dim, rank = factor[k].shape\n",
    "    if dim * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    if np.prod(sparse_tensor.shape) / dim * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    if dim * rank ** 2 > 1e+8:\n",
    "        vargin = 1\n",
    "    \n",
    "    if vargin == 0:\n",
    "        if k == 0:\n",
    "            var1 = kr_prod(factor[k + 2], factor[k + 1]).T\n",
    "        elif k == 1:\n",
    "            var1 = kr_prod(factor[k + 1], factor[k - 1]).T\n",
    "        elif k == 2:\n",
    "            var1 = kr_prod(factor[k - 1], factor[k - 2]).T\n",
    "        var2 = kr_prod(var1, var1)\n",
    "        var3 = (tau * (var2 @ ten2mat(binary_tensor, k).T).reshape([rank, rank, dim])\n",
    "                + np.dstack([var_Lambda_hyper] * dim))\n",
    "        var4 = (tau * var1 @ ten2mat(sparse_tensor, k).T\n",
    "                + np.dstack([var_Lambda_hyper @ var_mu_hyper] * dim)[0, :, :])\n",
    "        for i in range(dim):\n",
    "            var_Lambda = var3[:, :, i]\n",
    "            inv_var_Lambda = inv((var_Lambda + var_Lambda.T) / 2)\n",
    "            factor[k][i, :] = mvnrnd(inv_var_Lambda @ var4[:, i], inv_var_Lambda)\n",
    "    elif vargin == 1:\n",
    "        var_mu0 = var_Lambda_hyper @ var_mu_hyper\n",
    "        if k == 0:\n",
    "            for i in range(dim):\n",
    "                temp1 = np.zeros(rank)\n",
    "                temp2 = np.zeros((rank, rank))\n",
    "                pos0 = np.where(binary_tensor[k, :, :] != 0)\n",
    "                for t in range(len(pos0[0])):\n",
    "                    var = factor[k + 1][pos0[0][t], :] * factor[k + 2][pos0[1][t], :]\n",
    "                    temp1 += (var * sparse_tensor[k, pos0[0][t], pos0[1][t]])\n",
    "                    temp2 += np.outer(var, var)\n",
    "                var_Lambda = tau * temp2 + var_Lambda_hyper\n",
    "                inv_var_Lambda = inv((var_Lambda + var_Lambda.T) / 2)\n",
    "                var_mu = inv_var_Lambda @ (tau * temp1 + var_mu0)\n",
    "                factor[k][i, :] = mvnrnd(var_mu, inv_var_Lambda)\n",
    "        elif k == 1:\n",
    "            for i in range(dim):\n",
    "                temp1 = np.zeros(rank)\n",
    "                temp2 = np.zeros((rank, rank))\n",
    "                pos0 = np.where(binary_tensor[:, k, :] != 0)\n",
    "                for t in range(len(pos0[0])):\n",
    "                    var = factor[k - 1][pos0[0][t], :] * factor[k + 1][pos0[1][t], :]\n",
    "                    temp1 += (var * sparse_tensor[pos0[0][t], k, pos0[1][t]])\n",
    "                    temp2 += np.outer(var, var)\n",
    "                var_Lambda = tau * temp2 + var_Lambda_hyper\n",
    "                inv_var_Lambda = inv((var_Lambda + var_Lambda.T) / 2)\n",
    "                var_mu = inv_var_Lambda @ (tau * temp1 + var_mu0)\n",
    "                factor[k][i, :] = mvnrnd(var_mu, inv_var_Lambda)\n",
    "        elif k == 2:\n",
    "            for i in range(dim):\n",
    "                temp1 = np.zeros(rank)\n",
    "                temp2 = np.zeros((rank, rank))\n",
    "                pos0 = np.where(binary_tensor[:, :, k] != 0)\n",
    "                for t in range(len(pos0[0])):\n",
    "                    var = factor[k - 2][pos0[0][t], :] * factor[k - 1][pos0[1][t], :]\n",
    "                    temp1 += (var * sparse_tensor[pos0[0][t], pos0[1][t], k])\n",
    "                    temp2 += np.outer(var, var)\n",
    "                var_Lambda = tau * temp2 + var_Lambda_hyper\n",
    "                inv_var_Lambda = inv((var_Lambda + var_Lambda.T) / 2)\n",
    "                var_mu = inv_var_Lambda @ (tau * temp1 + var_mu0)\n",
    "                factor[k][i, :] = mvnrnd(var_mu, inv_var_Lambda)\n",
    "    return factor[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sampling precision term $\\tau$\n",
    "\n",
    "Draw $\\tau\\in\\text{Gamma}\\left(a^{*},b^{*}\\right)$ with following parameters:\n",
    "\n",
    "$$a^{*}=a_0+\\frac{1}{2}|\\Omega|,~b^{*}=b_0+\\frac{1}{2}\\sum_{(i,j,t)\\in\\Omega}\\left(y_{ijt}-\\sum_{s=1}^{r}u_{is}v_{js}x_{ts}\\right)^2.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_precision_tau(sparse_tensor, tensor_hat, pos_train, alpha, beta):\n",
    "    var_alpha = alpha + 0.5 * sparse_tensor[pos_train].shape[0]\n",
    "    var_beta = beta + 0.5 * np.sum((sparse_tensor - tensor_hat)[pos_train] ** 2)\n",
    "    return np.random.gamma(var_alpha, 1 / var_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sampling hyperparameters $\\left(\\boldsymbol{\\mu}_{u},\\Lambda_{u}\\right)$\n",
    "\n",
    "Draw\n",
    "\n",
    "- $\\Lambda_{u}\\sim\\mathcal{W}\\left(W_u^{*},\\nu_u^{*}\\right)$\n",
    "- $\\boldsymbol{\\mu}_{u}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{u}^{*},\\left(\\beta_u^{*}\\Lambda_u\\right)^{-1}\\right)$\n",
    "\n",
    "with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{u}^{*}=\\frac{m\\boldsymbol{\\bar{u}}+\\beta_0\\boldsymbol{\\mu}_0}{m+\\beta_0},~\\beta_u^{*}=m+\\beta_0,~\\nu_u^{*}=m+\\nu_0,$$\n",
    "$$\\left(W_u^{*}\\right)^{-1}=W_0^{-1}+mS_u+\\frac{m\\beta_0}{m+\\beta_0}\\left(\\boldsymbol{\\bar{u}}-\\boldsymbol{\\mu}_0\\right)\\left(\\boldsymbol{\\bar{u}}-\\boldsymbol{\\mu}_0\\right)^T,$$\n",
    "where $\\boldsymbol{\\bar{u}}=\\sum_{i=1}^{m}\\boldsymbol{u}_{i},~S_u=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\boldsymbol{u}_{i}-\\boldsymbol{\\bar{u}}\\right)\\left(\\boldsymbol{u}_{i}-\\boldsymbol{\\bar{u}}\\right)^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sampling hyperparameters $\\left(\\boldsymbol{\\mu}_{v},\\Lambda_{v}\\right)$\n",
    "\n",
    "Draw\n",
    "\n",
    "- $\\Lambda_{v}\\sim\\mathcal{W}\\left(W_v^{*},\\nu_v^{*}\\right)$\n",
    "- $\\boldsymbol{\\mu}_{v}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{v}^{*},\\left(\\beta_v^{*}\\Lambda_v\\right)^{-1}\\right)$\n",
    "\n",
    "with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{v}^{*}=\\frac{n\\boldsymbol{\\bar{v}}+\\beta_0\\boldsymbol{\\mu}_0}{n+\\beta_0},~\\beta_v^{*}=n+\\beta_0,~\\nu_v^{*}=n+\\nu_0,$$\n",
    "$$\\left(W_v^{*}\\right)^{-1}=W_0^{-1}+nS_v+\\frac{n\\beta_0}{n+\\beta_0}\\left(\\boldsymbol{\\bar{v}}-\\boldsymbol{\\mu}_0\\right)\\left(\\boldsymbol{\\bar{v}}-\\boldsymbol{\\mu}_0\\right)^T,$$\n",
    "where $\\boldsymbol{\\bar{v}}=\\sum_{j=1}^{n}\\boldsymbol{v}_{j},~S_v=\\frac{1}{n}\\sum_{j=1}^{n}\\left(\\boldsymbol{v}_{j}-\\boldsymbol{\\bar{v}}\\right)\\left(\\boldsymbol{v}_{j}-\\boldsymbol{\\bar{v}}\\right)^T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Sampling hyperparameters $\\left(\\boldsymbol{\\mu}_{x},\\Lambda_{x}\\right)$\n",
    "\n",
    "Draw\n",
    "\n",
    "- $\\Lambda_{x}\\sim\\mathcal{W}\\left(W_x^{*},\\nu_x^{*}\\right)$\n",
    "- $\\boldsymbol{\\mu}_{x}\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}_{x}^{*},\\left(\\beta_x^{*}\\Lambda_x\\right)^{-1}\\right)$\n",
    "\n",
    "with following parameters:\n",
    "\n",
    "$$\\boldsymbol{\\mu}_{x}^{*}=\\frac{f\\boldsymbol{\\bar{x}}+\\beta_0\\boldsymbol{\\mu}_0}{f+\\beta_0},~\\beta_x^{*}=f+\\beta_0,~\\nu_x^{*}=f+\\nu_0,$$\n",
    "$$\\left(W_x^{*}\\right)^{-1}=W_0^{-1}+fS_x+\\frac{f\\beta_0}{f+\\beta_0}\\left(\\boldsymbol{\\bar{x}}-\\boldsymbol{\\mu}_0\\right)\\left(\\boldsymbol{\\bar{x}}-\\boldsymbol{\\mu}_0\\right)^T,$$\n",
    "where $\\boldsymbol{\\bar{x}}=\\sum_{t=1}^{f}\\boldsymbol{x}_{t},~S_x=\\frac{1}{f}\\sum_{t=1}^{f}\\left(\\boldsymbol{x}_{t}-\\boldsymbol{\\bar{x}}\\right)\\left(\\boldsymbol{x}_{t}-\\boldsymbol{\\bar{x}}\\right)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hyperparameter(factor_k, mu0, beta0, W0, nu0):\n",
    "    dim = factor_k.shape[0]\n",
    "    mat_bar = np.mean(factor_k, axis = 0)\n",
    "    var_mu_hyper = (dim * mat_bar + beta0 * mu0) / (dim + beta0)\n",
    "    var_W_hyper = inv(inv(W0) + cov_mat(factor_k) + dim * beta0 / (dim + beta0)\n",
    "                      * np.outer(mat_bar - mu0, mat_bar - mu0))\n",
    "    var_Lambda_hyper = wishart(df = dim + nu0, scale = var_W_hyper, seed = None).rvs()\n",
    "    var_mu_hyper = mvnrnd(var_mu_hyper, inv((dim + beta0) * var_Lambda_hyper))\n",
    "    return var_mu_hyper, var_Lambda_hyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Performance Metrics\n",
    "\n",
    "- **RMSE**\n",
    "- **MAPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return  np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BGCP with `Numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGCP(dense_tensor, sparse_tensor, factor, burn_iter, gibbs_iter):\n",
    "    \"\"\"Bayesian Gaussian CP (BGCP) decomposition.\"\"\"\n",
    "    \n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    rank = factor[0].shape[1]\n",
    "    pos_train = np.where(sparse_tensor != 0)\n",
    "    pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    binary_tensor = np.zeros(dim)\n",
    "    binary_tensor[pos_train] = 1\n",
    "    \n",
    "    beta0 = 1\n",
    "    nu0 = rank\n",
    "    mu0 = np.zeros((rank))\n",
    "    W0 = np.eye(rank)\n",
    "    tau = 1\n",
    "    alpha = 1e-6\n",
    "    beta = 1e-6\n",
    "    \n",
    "    factor_plus = []\n",
    "    for k in range(len(dim)):\n",
    "        factor_plus.append(np.zeros((dim[k], rank)))\n",
    "    tensor_hat_plus = np.zeros(dim)\n",
    "    for it in range(burn_iter + gibbs_iter):\n",
    "        for k in range(len(dim)):\n",
    "            var_mu_hyper, var_Lambda_hyper = sample_hyperparameter(factor[k], mu0, beta0, W0, nu0)\n",
    "            factor[k] = sample_factor(sparse_tensor, binary_tensor, tau, factor, \n",
    "                                      var_mu_hyper, var_Lambda_hyper, k)\n",
    "        tensor_hat = cp_combine(factor)\n",
    "        tau = sample_precision_tau(sparse_tensor, tensor_hat, pos_train, alpha, beta)\n",
    "        if it + 1 > burn_iter:\n",
    "            factor_plus = [factor_plus[k] + factor[k] for k in range(len(dim))]\n",
    "            tensor_hat_plus += tensor_hat\n",
    "        if (it + 1) % 5 == 0 and it < burn_iter:\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "            print()\n",
    "        \n",
    "    factor = [i / gibbs_iter for i in factor_plus]\n",
    "    tensor_hat = tensor_hat_plus / gibbs_iter\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print()\n",
    "    \n",
    "    return tensor_hat, factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Organization\n",
    "\n",
    "### 1) Matrix Structure\n",
    "\n",
    "We consider a dataset of $m$ discrete time series $\\boldsymbol{y}_{i}\\in\\mathbb{R}^{f},i\\in\\left\\{1,2,...,m\\right\\}$. The time series may have missing elements. We express spatio-temporal dataset as a matrix $Y\\in\\mathbb{R}^{m\\times f}$ with $m$ rows (e.g., locations) and $f$ columns (e.g., discrete time intervals),\n",
    "\n",
    "$$Y=\\left[ \\begin{array}{cccc} y_{11} & y_{12} & \\cdots & y_{1f} \\\\ y_{21} & y_{22} & \\cdots & y_{2f} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{m1} & y_{m2} & \\cdots & y_{mf} \\\\ \\end{array} \\right]\\in\\mathbb{R}^{m\\times f}.$$\n",
    "\n",
    "### 2) Tensor Structure\n",
    "\n",
    "We consider a dataset of $m$ discrete time series $\\boldsymbol{y}_{i}\\in\\mathbb{R}^{nf},i\\in\\left\\{1,2,...,m\\right\\}$. The time series may have missing elements. We partition each time series into intervals of predifined length $f$. We express each partitioned time series as a matrix $Y_{i}$ with $n$ rows (e.g., days) and $f$ columns (e.g., discrete time intervals per day),\n",
    "\n",
    "$$Y_{i}=\\left[ \\begin{array}{cccc} y_{11} & y_{12} & \\cdots & y_{1f} \\\\ y_{21} & y_{22} & \\cdots & y_{2f} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ y_{n1} & y_{n2} & \\cdots & y_{nf} \\\\ \\end{array} \\right]\\in\\mathbb{R}^{n\\times f},i=1,2,...,m,$$\n",
    "\n",
    "therefore, the resulting structure is a tensor $\\mathcal{Y}\\in\\mathbb{R}^{m\\times n\\times f}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on London Movement Speed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "mask_rate = 0.20\n",
    "\n",
    "dense_mat = np.load('../datasets/London-data-set/hourly_speed_mat.npy')\n",
    "pos_obs = np.where(dense_mat != 0)\n",
    "num = len(pos_obs[0])\n",
    "sample_ind = np.random.choice(num, size = int(mask_rate * num), replace = False)\n",
    "sparse_mat = dense_mat.copy()\n",
    "sparse_mat[pos_obs[0][sample_ind], pos_obs[1][sample_ind]] = 0\n",
    "\n",
    "dense_tensor = dense_mat.reshape([dense_mat.shape[0], 30, 24])\n",
    "sparse_tensor = sparse_mat.reshape([sparse_mat.shape[0], 30, 24])\n",
    "del dense_mat, sparse_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Given only the partially observed data $\\mathcal{Y}\\in\\mathbb{R}^{m\\times n\\times f}$, how can we impute the unknown missing values?\n",
    "\n",
    "The main influential factors for such imputation model are:\n",
    "\n",
    "- `rank`.\n",
    "\n",
    "- `burn_iter`.\n",
    "\n",
    "- `gibbs_iter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5\n",
      "MAPE: 0.105037\n",
      "RMSE: 2.52473\n",
      "\n",
      "Iter: 10\n",
      "MAPE: 0.10257\n",
      "RMSE: 2.47023\n",
      "\n",
      "Iter: 15\n",
      "MAPE: 0.101983\n",
      "RMSE: 2.46156\n",
      "\n",
      "Iter: 20\n",
      "MAPE: 0.101535\n",
      "RMSE: 2.45217\n",
      "\n",
      "Iter: 25\n",
      "MAPE: 0.101146\n",
      "RMSE: 2.447\n",
      "\n",
      "Iter: 30\n",
      "MAPE: 0.100952\n",
      "RMSE: 2.44071\n",
      "\n",
      "Iter: 35\n",
      "MAPE: 0.100858\n",
      "RMSE: 2.43584\n",
      "\n",
      "Iter: 40\n",
      "MAPE: 0.100842\n",
      "RMSE: 2.43638\n",
      "\n",
      "Iter: 45\n",
      "MAPE: 0.100734\n",
      "RMSE: 2.4344\n",
      "\n",
      "Iter: 50\n",
      "MAPE: 0.100729\n",
      "RMSE: 2.43598\n",
      "\n",
      "Iter: 55\n",
      "MAPE: 0.100629\n",
      "RMSE: 2.43279\n",
      "\n",
      "Iter: 60\n",
      "MAPE: 0.100479\n",
      "RMSE: 2.42927\n",
      "\n",
      "Iter: 65\n",
      "MAPE: 0.100502\n",
      "RMSE: 2.43253\n",
      "\n",
      "Iter: 70\n",
      "MAPE: 0.100593\n",
      "RMSE: 2.43475\n",
      "\n",
      "Iter: 75\n",
      "MAPE: 0.100521\n",
      "RMSE: 2.4342\n",
      "\n",
      "Iter: 80\n",
      "MAPE: 0.100443\n",
      "RMSE: 2.42849\n",
      "\n",
      "Iter: 85\n",
      "MAPE: 0.100374\n",
      "RMSE: 2.43059\n",
      "\n",
      "Iter: 90\n",
      "MAPE: 0.10044\n",
      "RMSE: 2.43144\n",
      "\n",
      "Iter: 95\n",
      "MAPE: 0.100358\n",
      "RMSE: 2.42485\n",
      "\n",
      "Iter: 100\n",
      "MAPE: 0.100365\n",
      "RMSE: 2.42843\n",
      "\n",
      "Imputation MAPE: 0.0980458\n",
      "Imputation RMSE: 2.37571\n",
      "\n",
      "Running time: 522 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "dim = np.array(sparse_tensor[:10000, :, :].shape)\n",
    "rank = 10\n",
    "factor = []\n",
    "for k in range(len(dim)):\n",
    "    factor.append(0.1 * np.random.rand(dim[k], rank))\n",
    "burn_iter = 100\n",
    "gibbs_iter = 50\n",
    "BGCP(dense_tensor[:10000, :, :], sparse_tensor[:10000, :, :], factor, burn_iter, gibbs_iter)\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
